{
  "cache_config": {
    "version": "1.0",
    "max_size_gb": 2.0,
    "ttl_hours": 24,
    "compression": "gzip",
    "encryption": false,
    "backup_enabled": false
  },
  "embeddings_cache": {
    "type": "text_embeddings",
    "model": "sentence-transformers/all-MiniLM-L6-v2",
    "dimensions": 384,
    "index_type": "faiss",
    "similarity_metric": "cosine"
  },
  "reasoning_cache": {
    "type": "logical_chains",
    "max_depth": 10,
    "branching_factor": 5,
    "proof_storage": "compressed",
    "inference_patterns": "cached"
  },
  "memory_cache": {
    "episodic_memory": {
      "max_episodes": 1000,
      "compression": "temporal",
      "retrieval_index": "temporal+semantic"
    },
    "semantic_memory": {
      "knowledge_graph": "networkx",
      "embedding_dimension": 512,
      "relation_types": ["is_a", "part_of", "related_to", "causes"]
    }
  },
  "quantum_cache": {
    "circuit_cache": {
      "max_circuits": 100,
      "compilation_cache": true,
      "optimization_level": 3
    },
    "state_cache": {
      "max_states": 50,
      "fidelity_threshold": 0.99,
      "compression": "quantum_zip"
    }
  },
  "performance": {
    "arm64_optimized": true,
    "async_operations": true,
    "batch_processing": true,
    "memory_mapped": true
  }
}